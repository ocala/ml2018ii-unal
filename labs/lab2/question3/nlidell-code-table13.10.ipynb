{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, Bigrams, and Trigrams in Naive Bayes Classifiers \n",
    "# Using data from Table 13.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0L, u'Taipei Taiwan'],\n",
       "       [0L, u'Macao Taiwan Shanghai'],\n",
       "       [1L, u'Japan Sapporo'],\n",
       "       [1L, u'Sapporo Osaka Taiwan']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./table13.10.csv', usecols=[0,1], encoding='latin-1')\n",
    "df.columns = ['label','body']\n",
    "# label spam as 1, not spam as 0\n",
    "df['label'] = df['label'].replace([\"ham\",\"spam\"],[0,1])\n",
    "data = df.values\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngrams_bayes():\n",
    "    \n",
    "    def __init__(self, data, n=2, split=0.75):\n",
    "        \n",
    "        # split into training and testing data\n",
    "        self.train_data, self.test_data = train_test_split(data,\n",
    "                                                          train_size=split)\n",
    "        # convert into n grams\n",
    "        self.train_data = [[item[0], self.ngrams(n, item[1])] for item in self.train_data]\n",
    "        self.test_data = [[item[0], self.ngrams(n, item[1])] for item in self.test_data]\n",
    "        \n",
    "        # count unique n grams in training data\n",
    "        flattened = [gram for message in self.train_data for gram in message[1]]\n",
    "        self.unique = len(set(flattened))\n",
    "        \n",
    "        # init dicts\n",
    "        self.trainPositive = {}\n",
    "        self.trainNegative = {}\n",
    "        # counters\n",
    "        self.posGramCount = 0\n",
    "        self.negGramCount = 0\n",
    "        self.spamCount = 0\n",
    "        # priors\n",
    "        self.pA = 0\n",
    "        self.pNotA = 0\n",
    "        \n",
    "    def ngrams(self, n, text):\n",
    "        text = text.split(' ')\n",
    "        grams = []\n",
    "        for i in range(len(text)-n+1):\n",
    "            gram = ' '.join(text[i:i+n])\n",
    "            grams.append(gram)\n",
    "        return grams \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for item in self.train_data:\n",
    "            label = item[0]\n",
    "            grams = item[1]\n",
    "            if label == 1:\n",
    "                self.spamCount += 1   \n",
    "            for gram in grams:\n",
    "                if label == 1:\n",
    "                    self.trainPositive[gram] = self.trainPositive.get(gram, 0) + 1\n",
    "                    self.posGramCount += 1\n",
    "                else:\n",
    "                    self.trainNegative[gram] = self.trainNegative.get(gram, 0) + 1\n",
    "                    self.negGramCount += 1\n",
    "                    \n",
    "        self.pA = self.spamCount/float(len(self.train_data))\n",
    "        self.pNotA = 1.0 - self.pA\n",
    "        \n",
    "    def classify(self, text, alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text, 1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text, 0)\n",
    "        if (isSpam > notSpam):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def conditionalText(self, grams, label):\n",
    "        result = 1.0\n",
    "        for ngram in grams:\n",
    "            result *= self.conditionalNgram(ngram, label)\n",
    "        return result\n",
    "    \n",
    "    def conditionalNgram(self, ngram, label):\n",
    "        alpha = self.alpha\n",
    "        if label == 1:\n",
    "            return ((self.trainPositive.get(ngram,0)+alpha) /\n",
    "                    float(self.posGramCount+alpha*self.unique))\n",
    "        else:\n",
    "            return ((self.trainNegative.get(ngram,0)+alpha) /\n",
    "                    float(self.negGramCount+alpha*self.unique))\n",
    "            \n",
    "    def evaluate_test_data(self):\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            ruling = self.classify(text)\n",
    "            if ruling == label:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0) \n",
    "                \n",
    "        print(\"Evaluated {} test cases. {:.2f}% Accuracy\".format(len(results), 100.0*sum(results)/float(len(results))))\n",
    "        return sum(results)/float(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0L, [u'Macao', u'Taiwan', u'Shanghai']]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes = ngrams_bayes(data,1)\n",
    "unigram_bayes.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0L, [u'Taipei', u'Taiwan']],\n",
       " [1L, [u'Sapporo', u'Osaka', u'Taiwan']],\n",
       " [1L, [u'Japan', u'Sapporo']]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bayes.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1 test cases. 100.00% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1 test cases. 0.00% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_sms= ngrams_bayes(data,2) \n",
    "bigram_sms.train()\n",
    "bigram_sms.evaluate_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1 test cases. 0.00% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sms = ngrams_bayes(data,3) \n",
    "trigram_sms.train()\n",
    "trigram_sms.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our bayesian classifier performs well with unigrams, ok with bigrams, and is basically guessing randomly when it comes to trigrams. Increasing the size of your grams does not help this classifier classify this dataset. This is likely due to the data being made of up of short messages with highly specific and colloquial words. Nearly none of the trigrams will occur more than once in this dataset. I imagine that larger ngrams used in a baysian classifer would work well with something like product reviews which are longer than text messages and use less colloquial language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
